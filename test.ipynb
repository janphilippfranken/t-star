{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT tensor([[2, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2, 56, 33,  7],\n",
       "         [ 0,  5,  1, 34]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "class CustomSequential(nn.Sequential):\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        for module in self:\n",
    "            x = module(x, attention_mask)\n",
    "        return x\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x,  attention_mask=None):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        if attention_mask == None:\n",
    "          # print('attention mask is:', attention_mask)\n",
    "          wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        else:\n",
    "          # print(attention_mask)\n",
    "          wei = wei.masked_fill(attention_mask[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x,  attention_mask=None):\n",
    "        out = torch.cat([h(x,  attention_mask=attention_mask) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x,  attention_mask=None):\n",
    "        x = x + self.sa(self.ln1(x),  attention_mask=attention_mask)\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = CustomSequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx,  attention_mask, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x, attention_mask) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "        \n",
    "    def construct_mask(self, idx_thought, attention_mask, n_ahead):\n",
    "\n",
    "    \n",
    "      B, T, R = idx_thought.shape  # batch_size, seq_length, n_thoughts_ahead\n",
    "      \n",
    "      og_mask = torch.tril(torch.ones(T, T))\n",
    "      og_mask = og_mask.repeat(n_ahead, 1)\n",
    "      \n",
    "      # permute\n",
    "      idx_thought_perm = idx_thought.permute(0, 2, 1)  # (B, T, R) -> (B, R, T)\n",
    "      idx_thought_perm = idx_thought_perm.contiguous().view(B, R * T)  # (B, T, R) -> (B, R * T)\n",
    "\n",
    "      # print(\"idx_thought_perm\", idx_thought_perm.shape)\n",
    "      # print(\"idx_thought\", idx_thought)\n",
    "\n",
    "      # create masks\n",
    "      main_diagonal_mask = torch.eye(R * T)  # new diagonal masks\n",
    "      main_diagonal_mask[: attention_mask.shape[0], :attention_mask.shape[1]] += attention_mask\n",
    "     \n",
    "      # ugly eye addition\n",
    "      for i in range(T, T * R, T):\n",
    "        main_diagonal_mask[-i:, :i] += torch.eye(i)\n",
    "    \n",
    "      # augmenting based on og mask\n",
    "      main_diagonal_mask[:, :T] += og_mask\n",
    "      main_diagonal_mask[main_diagonal_mask > 1] = 1\n",
    "      # print(main_diagonal_mask)\n",
    "      return main_diagonal_mask, idx_thought_perm\n",
    "\n",
    "    def think(self, idx, attention_mask, max_new_tokens=1):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        idx_thought = idx.unsqueeze(-1) # add thought dimension R: (B, T) -> (B, T, R)\n",
    "\n",
    "        for thought_ahead in range(max_new_tokens):\n",
    "\n",
    "            if thought_ahead == 0:\n",
    "              \n",
    "              # compute logits\n",
    "              idx_cond = idx[:, -block_size:]\n",
    "              logits, loss = self(idx_cond, attention_mask)\n",
    "\n",
    "              # get and flatten probs \n",
    "              probs = F.softmax(logits, dim=-1) \n",
    "              probs = probs.view(B * T, -1) # flatten (B, T, C) -> (B*T, C)\n",
    "\n",
    "              # sample next thought token for each token in sequence\n",
    "              idx_next_thought = torch.multinomial(probs, num_samples=1) # (B*T, 1)\n",
    "              idx_next_thought = idx_next_thought.view(B, T) # (B*T, 1) -> (B, T)\n",
    "\n",
    "              # concatenate \n",
    "              idx_next_thought = idx_next_thought.unsqueeze(-1)\n",
    "              idx_thought = torch.cat((idx_thought, idx_next_thought), dim=-1) # (B, T+1)\n",
    "\n",
    "              # update attention_mask\n",
    "              attention_mask, idx_thought_permute = self.construct_mask(idx_thought, attention_mask, n_ahead=thought_ahead+2)\n",
    "            \n",
    "            else:\n",
    "\n",
    "              # compute logits\n",
    "              idx_cond = idx_thought_permute[:, -block_size:]\n",
    "              logits, loss = self(idx_cond, attention_mask)\n",
    "              logits = logits[:, -T:]\n",
    "\n",
    "              # get and flatten probs \n",
    "              probs = F.softmax(logits, dim=-1) \n",
    "              probs = probs.view(B * T, -1) # flatten (B, T, C) -> (B*T, C)\n",
    "\n",
    "              # sample next thought token for each token in sequence\n",
    "              idx_next_thought = torch.multinomial(probs, num_samples=1) # (B*T, 1)\n",
    "              idx_next_thought = idx_next_thought.view(B, T) # (B*T, 1) -> (B, T)\n",
    "\n",
    "              # concatenate \n",
    "              idx_next_thought = idx_next_thought.unsqueeze(-1)\n",
    "              idx_thought = torch.cat((idx_thought, idx_next_thought), dim=-1) # (B, T+1)\n",
    "\n",
    "              # update attention_mask\n",
    "              attention_mask, idx_thought_permute = self.construct_mask(idx_thought, attention_mask, n_ahead=thought_ahead+2)\n",
    "\n",
    "        return idx_thought\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "input = torch.randint(5, (1, 2))\n",
    "print(\"INPUT\", input)\n",
    "out = m.think(input, attention_mask=torch.tril(torch.ones(2, 2)), max_new_tokens=3)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (812658534.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    model knows answer -> just no thought and respond\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "model knows answer -> just no thought and respond\n",
    "model does not know answer? think [star-style but model can pick]\n",
    "and then talk?\n",
    "\n",
    "ie xxxx -> yyyy\n",
    "if probs of yyy sharp enough/top_k -> go ahead\n",
    "if not, think\n",
    "\n",
    "ie measure if the topk traj is the corr answer \n",
    "\n",
    "if not, think and take thoughts that lead to correct answer becoming the topk\n",
    "\n",
    "then finetune the thought head on that separately? \n",
    "\n",
    "so just finetune thought Head\n",
    "\n",
    "\n",
    "but also need to finetune the classifier/decision threshold thingy\n",
    "\n",
    "can use logits for that ? ie distributions of logits to predict -> 0, 1; try then to recognise this during training? or is there smth else that might be helpful here? smth before logits maybe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
